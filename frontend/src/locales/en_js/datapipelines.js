export const dataPipelines = {
  "toSel": "Please select",
  "toInput": "Please input",
  "noData": "No data available",
  "saveConfiguration": "Save Configuration",
  "dataAcquisition": "Data Acquisition",
  "dataCollectionTask": "Data Collection Task",
  "dataSourceManagement": "Data Source Management",
  "formatConversion": "Format Conversion",
  "dataFormatConversion": "Data Format Conversion",
  "taskDescription": "Task Description",
  "sourceFormat": "Source Format",
  "targetFormat": "Target Format",
  "dataFlowBranch": "Data Flow Branch",
  "startExecution": "Start Execution",
  "inProgress": "In Progress",
  "searchTaskName": "Search Task Name",
  "confirmTermination": "Confirm Termination",
  "terminate": "Terminate",
  "waiting": "Waiting",
  "error": "Error",
  "submitting": "Submitting",
  "taskStatus": "Task Status",
  "labelStudio": "Data Labeling",
  "dataSourceInfo": {
    "Mysql": {
      "title": "Relational Database (MySQL)",
      "desc": "Batch import database tables with custom tables and fields"
    },
    "Mongodb": {
      "title": "NoSQL Database (MongoDB)",
      "desc": "Import non-relational data with collection/field selection and schema conversion"
    },
    "File": {
      "title": "File Data Import",
      "desc": "Supports CSV, Excel, JSON and various file formats"
    },
    "Hive": {
      "title": "Hive System Import",
      "desc": "Efficiently read data stored in Hive systems"
    }
  },
  "testingConnection": "Testing connection",
  "submitting": "Submitting",
  "pleaseSelectAnExecutionTime": "Please select an execution time",
  "deletingTask": "Deleting task",
  "terminatingTask": "Terminating task",
  "createTask": "Create Task",
  "addDataSource": "Add Data Source",
  "fileFormat": "File Format",
  "connectionStatus": "Connection Status",
  "dataSourceType": "Data Source Type",
  "searchDataSources": "Search Data Sources",
  "searchNameOrDescription": "Search name",
  "dataProcessing": "Data Processing",
  "dataProcessingConfiguration": "Data Processing Configuration",
  "taskFlowConfiguration": "Task Flow Configuration",
  "dataExportConfiguration": "Data Export Configuration",
  "taskExecuted": "Task Executed",
  "taskExecutionFailed": "Task Execution Failed",
  "cannotCancel": "Task processing in progress, cannot cancel",
  "taskSuccessStop": "Task successfully stopped",
  "taskStopFailed": "Task stop failed",
  "processingResult": "Processing Result",
  "algorithmTemplate": "Algorithm Template",
  "builtInTemplate": "Built-in Template",
  "customTemplate": "Custom Template",
  "operatorManagement": "Operator Management",
  "systemDashboard": "System Dashboard",
  "concurrentTaskMonitoring": "Concurrent Task Monitoring",
  "myAlgorithmTemplate": "My Algorithm Templates",
  "createAlgorithmTemplate": "CreateAlgorithm Template",
  "dataProcessingDescription": "Data processing allows users to utilize different model operators to process data used by large models, including data cleaning, automatic data augmentation, and analysis. Users can obtain higher-quality data through data processing",
  "nodeName": "Node Name",
  "nodeConfig": "Node Config",
  "nodeType": "Node Type",
  "nodeNotSelected": "Node not selected",
  "fieldRequired": "{field} is required",
  "noNodesError": "No nodes exist in the workflow",
  "unnamedNode": "Unnamed node({id})",
  "unconnectedNodeError": "Node {nodeName}: not connected to any other nodes",
  "invalidConfigError": "Configuration for node {nodeName} is incomplete, please check required fields",
  "configRequiredError": "The '{configName}' field for node '{nodeName}' is mandatory, please fill it out completely",
  "saveError": "Error saving workflow",
  "noMatchingNodeFound": "No matching node found",
  "searchProcessing": "Search Processing Tasks",
  "zoomIn": "Zoom in",
  "zoomOut": "Zoom out",
  "resetView": "Reset view",
  "clearCanvas": "Clear canvas",
  "operationGuide": "Operation Guide",
  "operationGuide1": "Drag nodes from the left to the canvas area on the right",
  "operationGuide2": "Click on the connection point on the node and drag it to another node to create a connection",
  "operationGuide3": "Dragging nodes can adjust their position",
  "operationGuide4": "Hover over the node with the mouse to display the delete button",
  "operationGuide5": "Click the delete button or press the Delete key to delete the selected node",
  "configInfo": "Config Information",
  "taskLog": "Task Log",
  "search": "Search",
  "loading": "Loading",
  "taskCategories": "Task Categories",
  "allCategories": "All Categories",
  "taskList": "Task List",
  "taskName": "Task Name",
  "DatabaseName": "Database Name",
  "ServerAddress": "Server Address",
  "port": "Port Number",
  "username": "Username",
  "password": "Password",
  "authType": "Authentication Type",
  "collectionSourceName": "Collection Source Name",
  "server": "Server",
  "database": "Database",
  "task": "Task",
  "dataSourceDetails": "Data Source Details",
  "close": "Close",
  "operationSuccessful": "Operation Successful",
  "operationFailed": "Operation Failed",
  "basicInformation": "Basic Information",
  "dataSourceName": "Data Source Name",
  "lastUpdate": "Last Update",
  "normal": "Normal",
  "toBeTested": "To be tested",
  "anomaly": "Anomaly",
  "useRecord": "Usage Record",
  "dataImportTask": "Data Import Task",
  "persons": "task",
  "recentlyUsed": "Recently Used",
  "dataVolume": "Data Volume",
  "total": "Total",
  "startAt": "Started at",
  "done": "Completed",
  "taskRunningHost": "Task Running Host",
  "recordsHaveBeenImported": "Records Imported",
  "totalNumberOfRecords": "Total Number of Records",
  "cancelTask": "Cancel Task",
  "refreshStatus": "Refresh Status",
  "viewLog": "View Log",
  "RunItAgain": "Run Again",
  "dataConnectionConfiguration": "Data Connection Configuration",
  "authType_option_NONE": "No identity verification",
  "authType_option_LDAP": "Use LDAP/AD-based user identity verification",
  "authType_option_KERBEROS": "Use Kerberos/GSSAPI for identity verification",
  "authType_placeholder": "Currently only LDAP mode is supported",
  "collectionSourceDesc": "Collection Source Description",
  "testLink": "Test Connection",
  "dataFilteringConfiguration": "Data Filtering Configuration",
  "selectionSet": "Selection Set",
  "searchForTheTableName": "Search Table Name",
  "allFields": "All Fields",
  "selectAll": "Select All",
  "saveTheConfiguration": "Save Configuration",
  "saveAndExecute": "Save and Execute",
  "executeImmediately": "Execute Immediately",
  "selectTheExecutionTime": "Select Execution Time",
  "sure": "Confirm",
  "PleaseSelectTime": "Please Select Time",
  "fileUpload": "File Upload",
  "jumpLink": "Jump Link",
  "linkSuccess": "Connection Successful",
  "linkError": "Connection Failed",
  "connectionInformation": "Connection information",
  "TaskFailed": "Task Failed",
  "manualStop": "Manual stop",

  "createTime": "Creation Time",
  "dataAmount": "Data Amount",
  "finishTime": "Completion Time",
  "processedDataAmount": "Processed Data Amount",
  "processInfo": "Processing Details",
  "processStatus": "Running Status",
  "processedData": "Processed Data",
  "graphicDemonstration": "Graphic Demonstration",
  "sessionProcessedResult": "Session Processing Result",
  "index": "Serial Number",
  "preSession": "Session Before Processing",
  "processType": "Processing Method",
  "afterSession": "Session After Processing",
  "taskLog": "Task Log",
  "logName": "Log Name",
  "downloadLog": "Download Log",
  "others": "Others",
  "replace": "Replace",
  "deduplicate": "Deduplicate",
  "remove": "Remove",
  "data_refine": "Data Refinement",
  "data_generation": "Data Generation",
  "data_enhancement": "Data Enhancement",

  "taskType": "Task Type",
  "dataCleaning": "Data Cleaning",
  "processingStatus": "Processing Status",
  "processingText": "Processing Field",
  "inProgress": "In Progress",
  "completed": "Completed",
  "dataSource": "Data Source",
  "dataSourceBranch": "Data Source Branch",
  "dataFlow": "Data Flow",
  "startTime": "Start Time",
  "endTime": "End Time",
  "executionStatus": {
    "success": "Success",
    "error": "Failed",
    "processing": "Processing",
    "wainting": "Pending",
  },
  "unknown": "Unknown",
  "online": "Online",
  "offline": "Offline",
  "operations": "Operations",
  "delete": "Delete",
  "deleteConfirm": "Confirm Delete",
  "cancelConfirm": "Confirm Cancellation",
  "rerunItConfirm": "Confirm Rerun",
  "execute": "Execute",
  "cancelExecute": "Cancel Execution",
  "executeConfirm": "Confirm Execution",
  "confirm": "Confirm",
  "reset": "Replace",
  "details": "Details",
  "authorize": "authorize",
  "operatorAuthorization": "Operator Authorization",
  "SearchUserName": "Search User Name",
  "SearchOrganizationName": "Search Organization Name",
  "person": "person",
  "organization": "organization",
  "selected": "Selected",
  "editIcon": "Edit Icon",
  "iconPreview": "Icon Preview",
  "please": "Please",
  "uploadTips1": "Support JPG and PNG format icons, up to 10MB",
  "uploadTips2": "A new icon has been uploaded, you can continue to upload and replace or click OK to save.",
  "uploadIcon": "Upload Icon",
  "uploadStatusTips1": "Click or drag the icon here",
  "uploadStatusTips2": "The current icon will be replaced after uploading",
  "uploading": "Uploading",
  "uploadSuccess": "Upload Success",
  "uploadSuccessTips1": "You can continue to upload replacements or click OK to save.",
  "reUpload": "Re-upload",
  "uploadFailed": "Upload Failed",
  "retry": "Retry",
  "uploadSuccessTips2": "Icon uploaded successfully",
  "uploadFailedTips1": "Please upload an icon in JPG or PNG format",
  "uploadFailedTips2": "The icon size cannot exceed 10MB.",
  "uploadFailedTips3": "Upload failed, please try again",
  "networkError": "Network error, please check the connection and try again",
  "submitting": "Submitting",
  "algorithmTemplateDescription": "Algorithm templates allow users to use various model operators to form workflows for data cleaning, automatic data enhancement, analysis, and other tasks.",
  "taskTemplate": "Task Template",
  "searchTaskTemplate": "Search task template",
  "searchTemplate": "Search Template",
  "searchOperator": "search operator",
  "templateName": "Template Name",
  "templateDescription": "Template Description",
  "searchTaskTemplate": "Search Task Template",
  "searchOperator": "Search Operator",
  "nextStep": "Next Step",
  "previousStep": "Previous Step",
  "create": "Create",
  "edit": "Edit",
  "type": "Type",
  "copy": "Copy",
  "use": "Use",
  "templateList": "Template List",
  "createTemplate": "Create Template",
  "editTemplate": "Edit Template",
  "general": "General",
  "dataCleaningDescription": "Clean data using multiple operators such as deduplication and desensitization to meet usage requirements",
  "dataAugmentation": "Data Augmentation",
  "dataAugmentationDescription": "Automatically generate more data based on seed data, which can be used for training data generation, supporting custom parameters and Prompts",
  "textClassification": "Text Classification",
  "textClassificationDescription": "Enhance training data for text classification tasks, applicable to scenarios such as sentiment classification, label classification, product classification, etc.",
  "textExtraction": "Text Extraction",
  "textExtractionDescription": "Enhance training data for text extraction tasks, applicable to scenarios such as specific format extraction, entity extraction, feature extraction, etc.",
  "textGeneration": "Text Creation",
  "textGenerationDescription": "Enhance training data for text creation tasks, applicable to scenarios such as news writing, advertisement generation, stylized writing content, etc.",
  "apply": "Use",
  "newTask": "New Task",
  "pushToOriginalDataset": "Push to Original Dataset",
  "pushToOriginalDatasetDescription": "After pushing to the original dataset, it will be pushed to the original dataset repo as a new submission",
  "pushToNewDataset": "Push to New Dataset",
  "pushToSelectedDatasetDescription": "After data cleaning is completed, it will be pushed to the selected dataset",
  "targetDataset": "Target Dataset Name",
  "predefinedOperatorSelection": "Predefined Operator Selection",
  "predefinedOperator": "Predefined Operator",
  "peratorTip": "Currently supports multiple predefined operators of types Mapper, Filter, and Deduplicator",
  "publishAsNewTemplate": "Publish as New Template",
  "executionOrder": "Execution Order",
  "enableOrNot": "Enable or Not",
  "addOperator": "Add Operator",
  "operatorType": "Operator Type",
  "operatorName": "Operator Name",
  "textNormalization": "Text Normalization",
  "removeSpecialContent": "Remove Special Content",
  "maskSensitiveInformation": "Mask Sensitive Information",
  "specialCharacterRatioFiltering": "Special Character Ratio Filtering",
  "sensitiveWordFiltering": "Sensitive Word Filtering",
  "nGramRepetitionRatioFiltering": "N-Gram Repetition Ratio Filtering",
  "lengthFiltering": "Length Filtering",
  "md5Deduplication": "MD5 Deduplication",
  "articleSimilarityDeduplication": "Article Similarity Deduplication",
  "toxicityRemoval": "Toxicity Removal",
  "operatorConfiguration": "Operator Configuration",
  "unicodeTextNormalization": "Unicode Text Normalization",
  "convertTraditionalChineseToSimplifiedChinese": "Convert Traditional Chinese to Simplified Chinese",
  "removeURLLinks": "Remove URL Links",
  "removeInvisibleCharacters": "Remove Invisible Characters",
  "removeHtmlTagsAndParseHtmlContent": "Remove HTML Formatting Characters and Parse HTML Text",
  "maximumRatio": "Maximum Ratio",
  "lengthN": "Length N",
  "minimumLength": "Minimum Length",
  "characters": "Characters",
  "windowLength": "Window Length",
  "description": "Description",
  "textNormalizationDesc": "Text Unicode normalization and conversion from Traditional Chinese to Simplified Chinese",
  "removeSpecialContentDesc": "Remove special content in the text, such as URLs, invisible characters, HTML formatting characters, etc.",
  "maskSensitiveInformationDesc": "Mask sensitive information, such as replacing email address characters with [EMAIL], phone numbers with [TELEPHONE] or [MOBILEPHONE], ID numbers with [IDNUM]",
  "specialCharacterRatioFilteringDesc": "Filter text based on the proportion of special characters. Keep samples where the number of special characters (including punctuation marks, numbers, spaces, emojis, etc.) accounts for no more than the set threshold of the total text length. Data samples exceeding the set proportion will be filtered out",
  "sensitiveWordFilteringDesc": "Filter out samples with sensitive words",
  "nGramRepetitionRatioFilteringDesc": "Keep samples where the character-level N-Gram repetition ratio does not exceed the set threshold. Samples exceeding the threshold will be filtered out",
  "lengthFilteringDesc": "Filter data based on text length. Data outside the length range will be filtered out",
  "md5DeduplicationDesc": "Deduplicate by comparing MD5 values generated from the text. Samples with consistent MD5 checksums will be filtered out",
  "articleSimilarityDeduplicationDesc": "Calculate the similarity between texts using the SimHash algorithm. Samples with similarity exceeding the threshold will be filtered out",
  "toxicityRemovalDesc": "Automatically detect, analyze, and remove sensitive and non-compliant content in the data. This operator only analyzes and processes the data content and does not save or retain any data content before or after processing",
  "previewBefore": "Effect Preview (Before Cleaning)",
  "previewAfter": "Effect Preview (After Cleaning)",
  "creationCompleted": "Creation Completed",
  "updateTemplate": "Update Template",
  "cancel": "Cancel",
  "templateNameExists": "The template name already exists. Please use another name",
  "Queued": "Pending",
  "Processing": "Processing",
  "Finished": "Completed",
  "Failed": "Failed",
  "Timeout": "Timeout",
  "sessionDel": "Session Deleted",

  "toolsTit": "Tool Pool",
  "toolsDec": "Dataflow Tool Pool is an all-in-one multimodal data processing system that can make data higher quality, more valuable, and more suitable for large model processing.",
  "toolsSearch": "Search Tools",
  "toolsType": "Tool Category",
  "toolsName": "Tool Name",
  "toolsUse": "Use Tool",
  "taskType1": "Operator",
  "taskType2": "Tool",
  "log": "Log",
  "toolsTab1": "Internal Tools",
  "toolsTab2": "External Tools",

  "analysis_common_internal": "General Analysis Tool",
  "dataset_spliter_by_language_preprocess_internal": "Dataset Splitting by Language Preprocessing Tool",
  "prepare_dataset_from_repo_preprocess_internal": "Dataset Preparation from Repository Preprocessing Tool",
  "raw_alpaca_cot_merge_add_meta_preprocess_internal": "Raw Alpaca-Cot Data Merging and Metadata Addition Preprocessing Tool",
  "raw_arxiv_to_jsonl_preprocess_internal": "Raw arXiv Data Conversion to JSONL Preprocessing Tool",
  "raw_stackexchange_to_jsonl_preprocess_internal": "Raw Stack Exchange Data Conversion to JSONL Preprocessing Tool",
  "reformat_csv_nan_value_preprocess_internal": "CSV File NaN Value Reformatting Preprocessing Tool",
  "reformat_jsonl_nan_value_preprocess_internal": "JSONL File NaN Value Reformatting Preprocessing Tool",
  "serialize_meta_preprocess_internal": "Metadata Serialization Preprocessing Tool",
  "count_token_postprocess_internal": "Token Counting Postprocessing Tool",
  "data_mixture_postprocess_internal": "Data Mixing Postprocessing Tool",
  "deserialize_meta_postprocess_internal": "Metadata Deserialization Postprocessing Tool",
  "quality_classifier_common_internal": "Quality Classifier General",
  "opencsg_data_extraction_preprocess_internal": "Open Computing System Data Extraction Preprocessing",
  "opencsg_scrape_url_data_preprocess_internal": "Open Computing System URL Data Scraping Preprocessing",

  "analysis_common_internal_dec": "This analyzer class is used to analyze specific datasets. It calculates statistics for all filtering operations in the configuration file, applies various analyses (such as overall analysis, column-by-column analysis, etc.) to these statistics, and generates analysis results (statistical tables, distribution charts, etc.) to help users better understand the input dataset.",
  "dataset_spliter_by_language_preprocess_internal_dec": "Load the dataset from the source directory, then use the operation filter named LanguageIDScoreFilter for language identification, and finally split the dataset by language and save it.",
  "prepare_dataset_from_repo_preprocess_internal_dec": "Prepare datasets from code repositories, including: repository name, file path in the repository, and file content.",
  "raw_alpaca_cot_merge_add_meta_preprocess_internal_dec": "Convert raw Alpaca-Cot data downloaded from Hugging Face into JSONL files, merge instruction/input/output texts, and add metadata information.",
  "raw_arxiv_to_jsonl_preprocess_internal_dec": "Convert raw arXiv data (gzipped tar files) to JSONL format.",
  "raw_stackexchange_to_jsonl_preprocess_internal_dec": "Convert raw Stack Exchange data downloaded from Archive (reference: https://archive.org/download/stackexchange) into multiple JSONL files.",
  "reformat_csv_nan_value_preprocess_internal_dec": "Use Hugging Face to load CSV or TSV files that may contain NaN values, and can be processed by setting additional parameters (such as setting keep_default_na to False).",
  "reformat_jsonl_nan_value_preprocess_internal_dec": "Reformat JSONL files that may contain NaN values. Traverse the JSONL file, find the first object that does not contain NaN as the reference feature type, and set it as the benchmark when loading all JSONL files.",
  "serialize_meta_preprocess_internal_dec": "Serialize all fields in the JSONL file except those specified by the user to ensure that the dataset can still be loaded normally even if the text format of each line in the JSONL file is inconsistent.",
  "count_token_postprocess_internal_dec": "Count the number of tokens for a given dataset and tokenizer. Currently, only JSONL format is supported.",
  "data_mixture_postprocess_internal_dec": "Mix multiple datasets into one dataset. Randomly select samples from each dataset and mix these samples, then export them as a new mixed dataset. Supported formats include: [\"jsonl\", \"json\", \"parquet\"].",
  "deserialize_meta_postprocess_internal_dec": "Deserialize specified fields in the JSONL file.",
  "quality_classifier_common_internal_dec": "This quality classifier class is used to predict the scores of documents in the dataset. It will calculate scores for all rows and provide two columns for each row: score and should_keep, to help users decide which row should be deleted. By default, if the score is higher than 0.9, the row will be marked as should_keep=1.",
  "opencsg_data_extraction_preprocess_internal_dec": "A high-quality tool for converting PDF to Markdown and JSON",
  "opencsg_scrape_url_data_preprocess_internal_dec": "A large language model-based data scraping tool for websites and local documents (XML, HTML, JSON, etc.)",
}