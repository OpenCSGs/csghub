export const dataPipelines = {
  "toSel": "Please select ",
  "toInput": "Please Input ",
  "noData": "No data available.",
  "dataProcessing": "Data Processing",
  "processingResult": "Processing Result",
  "algorithmTemplate": "Algorithm Template",
  "myAlgorithmTemplate": "My Algorithm Template",
  "dataProcessingDescription": "Data processing allows users to leverage different model operators to handle data used in LLMs. This includes data cleaning, automatic data augmentation, and analysis, enabling users to obtain higher quality data.",
  "searchProcessing": "Search processing task",
  "search": "Search",
  "taskCategories": "Task Categories",
  "allCategories": "All categories",
  "createTask": "Create Task",
  "taskList": "Task List",
  "taskName": "Task Name",

  "createTime": "Creation Time",
  "dataAmount": "Data Amount",
  "finishTime": "Completion Time",
  "processedDataAmount": "Processed Data Amount",
  "processInfo": "Processing Details",
  "processStatus": "Running Status",
  "processedData": "Processed Data",
  "sessionProcessedResult": "Session Processing Result",
  "index": "Index",
  "preSession": "Before Processing Session",
  "processType": "Processing Method",
  "afterSession": "After Processing Session",
  "taskLog": "Task Log",
  "logName": "Log Name",
  "downloadLog": "Download Log",
  "others": "Others",
  "replace": "Replace",
  "deduplicate": "Deduplicate",
  "remove": "Remove",
  "data_refine": "data_refine",
  "data_generation": "data_generation",
  "data_enhancement": "data_enhancement",

  "taskType": "Task Type",
  "dataCleaning": "Data Cleaning",
  "processingStatus": "Processing Status",
  "processingText": "Processing Text",
  "inProgress": "In Progress",
  "completed": "Completed",
  "dataSource": "Data Source",
  "dataSourceBranch": "Data Source Branch",
  "dataFlow": "Data Flow",
  "startTime": "Start Time",
  "operations": "Operations",
  "delete": "Delete",
  "deleteConfirm": "Confirm Delete",
  "confirm": "Confirm",
  "reset": "Replace",
  "details": "Details",
  "algorithmTemplateDescription": "The algorithm template allows users to build workflows using various model operators, enabling tasks such as data cleaning, automated data augmentation, and analysis.",
  "taskTemplate": "Task Template",
  "searchTaskTemplate": "Search task template",
  "templateName": "Template Name",
  "templateDescription": "Template Description",
  "peratorTip": "Supports various preset operators: Mapper, Filter, and Deduplicator.",
  "create": "Create",
  "edit": "Edit",
  "type": "Type",
  "createTemplate": "Create Template",
  "editTemplate": "Edit Template",
  "general": "General",
  "dataCleaningDescription": "Clean data using various operators such as deduplication and desensitization, ensuring it meets the required standards for use.",
  "dataAugmentation": "Data Augmentation",
  "dataAugmentationDescription": "Automatically generate more data from seed data for training, with support for custom parameters and prompts.",
  "textClassification": "Text Classification",
  "textClassificationDescription": "Augment training data for text classification tasks, applicable to scenarios such as sentiment classification, tag classification, and product classification.",
  "textExtraction": "Text Extraction",
  "textExtractionDescription": "Augment training data for text extraction tasks, applicable to scenarios such as specific format extraction, entity extraction, and key element extraction.",
  "textGeneration": "Text Generation",
  "textGenerationDescription": "Augment training data for text generation tasks, applicable to scenarios such as news writing, ad copy generation, and content stylization.",
  "apply": "Apply",
  "newTask": "New Task",
  "pushToOriginalDataset": "Push to original dataset",
  "pushToOriginalDatasetDescription": "Once data cleaning is completed, it will be pushed to the original dataset repo as a new submission",
  "pushToNewDataset": "Push to new dataset",
  "pushToSelectedDatasetDescription": "Once data cleaning is completed, it will be pushed to the selected dataset",
  "targetDataset": "Target dataset name",
  "predefinedOperatorSelection": "Predefined Operator Selection",
  "predefinedOperator": "Predefined Operator",
  "publishAsNewTemplate": "Publish as New Template",
  "executionOrder": "Execution Order",
  "enableOrNot": "Enable or Not",
  "addOperator": "Add Operator",
  "operatorType": "Operator Type",
  "operatorName": "Operator Name",
  "textNormalization": "Text Normalization",
  "removeSpecialContent": "Remove Special Content",
  "maskSensitiveInformation": "Mask Sensitive Information",
  "specialCharacterRatioFiltering": "Special Character Ratio Filtering",
  "sensitiveWordFiltering": "Sensitive Word Filtering",
  "nGramRepetitionRatioFiltering": "N-Gram Repetition Ratio Filtering",
  "lengthFiltering": "Length Filtering",
  "md5Deduplication": "MD5 Deduplication",
  "articleSimilarityDeduplication": "Article Similarity Deduplication",
  "toxicityRemoval": "Toxicity Removal",
  "operatorConfiguration": "Operator Configuration",
  "unicodeTextNormalization": "Unicode text normalization",
  "convertTraditionalChineseToSimplifiedChinese": "Convert Traditional Chinese to Simplified Chinese",
  "removeURLLinks": "Remove URL Links",
  "removeInvisibleCharacters": "Remove invisible characters",
  "removeHtmlTagsAndParseHtmlContent": "Remove html tags and parse html content",
  "maximumRatio": "Maximum ratio",
  "lengthN": "Length N",
  "minimumLength": "Minimum length",
  "characters": "Characters",
  "windowLength": "Window Length",
  "description": "Description",
  "textNormalizationDesc": "Unicode text normalization and conversion from Traditional Chinese to Simplified Chinese",
  "removeSpecialContentDesc": "Remove special content from the text, such as URLs, invisible characters, and html tags.",
  "maskSensitiveInformationDesc": "Mask sensitive information, such as replacing email addresses with [EMAIL], phone numbers with [TELEPHONE] or [MOBILEPHONE], and ID numbers with [IDNUM].",
  "specialCharacterRatioFilteringDesc": "Filter text based on the ratio of special characters. Keep samples where the number of special characters does not exceed a set threshold of the total text length. Special characters include punctuation marks, numbers, spaces, and emojis. Samples exceeding the set ratio will be filtered out.",
  "sensitiveWordFilteringDesc": "Filter out samples containing sensitive words.",
  "nGramRepetitionRatioFilteringDesc": "Keep samples where the character-level N-Gram repetition ratio does not exceed the set threshold. Samples exceeding the threshold will be filtered out.",
  "lengthFilteringDesc": "Filter data based on text length. Samples outside the length range will be filtered out.",
  "md5DeduplicationDesc": "Deduplicate samples by comparing their MD5 hash. Samples with matching MD5 values will be filtered out.",
  "articleSimilarityDeduplicationDesc": "Use the SimHash algorithm to calculate text similarity. Samples exceeding the similarity threshold will be filtered out.",
  "toxicityRemovalDesc": "Automatically detect, analyze, and remove sensitive or non-compliant content from the data. This operator only analyzes and processes the content and does not save or keep any data before or after processing.",
  "previewBefore": "Preview (Before Cleaning)",
  "previewAfter": "Preview (After Cleaning)",
  "creationCompleted": "Creation Completed",
  "cancel": "Cancel",
  "templateNameExists": "Template name already exists, please use a different name",
  "Queued": "Queued",
  "Processing": "Processing",
  "Finished": "Finished",
  "Failed": "Failed",
  "Timeout": "Timeout",
  "sessionDel": "Session has been deleted",

  "toolsTit": "Tool Pool",
  "toolsDec": "The Dataflow Tool Pool is a one-stop, multi-modal data processing system that enhances data quality, increases value, and makes it more suitable for large model processing.",
  "toolsSearch": "Search Tools",
  "toolsType": "Tool Categories",
  "toolsName": "Tool Name",
  "toolsUse": "Use Tool",
  "taskType1": "Operator",
  "taskType2": "Tool",
  "log": "Log",
  "toolsTab1": "Internal Tools",
  "toolsTab2": "External Tools",

  "analysis_common_internal": "analysis common",
  "dataset_spliter_by_language_preprocess_internal": "dataset spliter by language preprocess",
  "prepare_dataset_from_repo_preprocess_internal": "prepare dataset from repo preprocess",
  "raw_alpaca_cot_merge_add_meta_preprocess_internal": "raw alpaca cot merge add meta preprocess",
 "raw_arxiv_to_jsonl_preprocess_internal": "raw arxiv to jsonl preprocess",
  "raw_stackexchange_to_jsonl_preprocess_internal": "raw stackexchange to jsonl preprocess",
  "reformat_csv_nan_value_preprocess_internal": "reformat csv nan value preprocess",
  "reformat_jsonl_nan_value_preprocess_internal": "reformat jsonl nan value preprocess",
  "serialize_meta_preprocess_internal": "serialize meta preprocess",
  "count_token_postprocess_internal": "count token postprocess",
  "data_mixture_postprocess_internal": "data mixture postprocess",
  "deserialize_meta_postprocess_internal": "deserialize meta postprocess",
  "quality_classifier_common_internal": "quality classifier common",

  "opencsg_data_extraction_preprocess_internal": "opencsg data extraction preprocess",
  "opencsg_scrape_url_data_preprocess_internal": "opencsg scrape url data preprocess",

  "analysis_common_internal_dec": "This Analyzer class is used to analyze a specific dataset. It will compute stats for all filter ops in the config file, apply multiple analysis (e.g. OverallAnalysis, ColumnWiseAnalysis, etc.) on these stats, and generate the analysis results (stats tables, distribution figures, etc.) to help users understand the input dataset better.",
  "dataset_spliter_by_language_preprocess_internal_dec": "Load dataset from the source directory, then apply language identification using the operation filter called LanguageIDScoreFilter, finally, split the dataset by language and save it.",
  "prepare_dataset_from_repo_preprocess_internal_dec": "Prepare dataset from code repo with format like this: Repository Name, Filepath in the Repository, File Contents.",
  "raw_alpaca_cot_merge_add_meta_preprocess_internal_dec": "This tool is used for converting the raw Alpaca-Cot data downloaded from HuggingFace to jsonl files, merge instruction/input/output to text for process, and add meta info.",
  "raw_arxiv_to_jsonl_preprocess_internal_dec": "Convert the raw arXiv data (gzipped tar file) into the jsonl format.",
  "raw_stackexchange_to_jsonl_preprocess_internal_dec": "Convert the raw Stack Exchange data downloaded from Archive (ref: https://archive.org/download/stackexchange) to several jsonl files.",
  "reformat_csv_nan_value_preprocess_internal_dec": "Reformat csv or tsv files that may contain Nan values using HuggingFace to load with extra args, e.g. set keep_default_na to False.",
  "reformat_jsonl_nan_value_preprocess_internal_dec": "Reformat the jsonl files which may contain Nan values. Traverse jsonl files to find the first object that does not contain Nan as a reference feature type, then set it for loading all jsonl files.",
  "serialize_meta_preprocess_internal_dec": "Serialize all the fields in the jsonl file except the fields specified by users to ensure that the jsonl file with inconsistent text format for each line can also be load normally by the dataset.",
  "count_token_postprocess_internal_dec": "Count the number of tokens for given dataset and tokenizer. Only support 'jsonl' now.",
  "data_mixture_postprocess_internal_dec": "Mix multiple datasets into one dataset. Randomly select samples from every dataset and mix these samples, then export to a new mixed dataset. Supported suffixes include: [\"jsonl\", \"json\", \"parquet\"].",
  "deserialize_meta_postprocess_internal_dec": "Deserialize the specified field in the jsonl file.",
  "quality_classifier_common_internal_dec": "This Quality Classifier class is used to predict document scores on dataset.It will compute scores for all rows, and give 2 columns: score and should_keep for each row to help user decide which row should be removed. By default, mark row as should_keep=1 if score is higher than 0.9.",
  "opencsg_data_extraction_preprocess_internal_dec": "A high-quality tool for convert PDF to Markdown and JSON",
  "opencsg_scrape_url_data_preprocess_internal_dec": "Data scrape tool based on large language model for websites and native documents (XML, HTML, JSON, etc.).",

}
